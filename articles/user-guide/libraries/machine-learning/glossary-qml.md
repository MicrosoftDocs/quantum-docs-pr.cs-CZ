---
title: Knihovna pro kvantové strojové učení
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
ms.openlocfilehash: f9b33a607a892179795d0700ba3080f9a24ab94a
ms.sourcegitcommit: 0181e7c9e98f9af30ea32d3cd8e7e5e30257a4dc
ms.translationtype: MT
ms.contentlocale: cs-CZ
ms.lasthandoff: 06/23/2020
ms.locfileid: "85274721"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="7fefc-102">Machine Learning Glosář</span><span class="sxs-lookup"><span data-stu-id="7fefc-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="7fefc-103">Školení klasifikátoru, zaměřené na okruhy, je proces s mnoha pohybujícími se částmi, které vyžadují stejné (nebo mírně větší) množství kalibrace v rámci zkušební verze a chyby jako školení tradičních klasifikátorů.</span><span class="sxs-lookup"><span data-stu-id="7fefc-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="7fefc-104">Tady definujeme hlavní koncepty a složky tohoto školicího procesu.</span><span class="sxs-lookup"><span data-stu-id="7fefc-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="7fefc-105">Plány školení a testování</span><span class="sxs-lookup"><span data-stu-id="7fefc-105">Training/testing schedules</span></span>

<span data-ttu-id="7fefc-106">V kontextu školení třídění *plán* popisuje podmnožinu ukázek dat v celkovém školení nebo testovací sadě.</span><span class="sxs-lookup"><span data-stu-id="7fefc-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="7fefc-107">Plán je obvykle definován jako kolekce ukázkových indexů.</span><span class="sxs-lookup"><span data-stu-id="7fefc-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="7fefc-108">Skóre parametru nebo posunu</span><span class="sxs-lookup"><span data-stu-id="7fefc-108">Parameter/bias scores</span></span>

<span data-ttu-id="7fefc-109">Vzhledem ke vektoru parametru kandidáta a posunu klasifikátoru se jejich *skóre ověření* měří vzhledem k zvolenému plánu ověřování S a je vyjádřeno počtem chybných klasifikací zjištěných ve všech ukázkách v plánu S.</span><span class="sxs-lookup"><span data-stu-id="7fefc-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="7fefc-110">Hyperparameters</span><span class="sxs-lookup"><span data-stu-id="7fefc-110">Hyperparameters</span></span>

<span data-ttu-id="7fefc-111">Proces školení modelů se řídí některými předem nastavenými hodnotami nazvanými *parametry*:</span><span class="sxs-lookup"><span data-stu-id="7fefc-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="7fefc-112">Rychlost učení</span><span class="sxs-lookup"><span data-stu-id="7fefc-112">Learning rate</span></span>

<span data-ttu-id="7fefc-113">Jedná se o jeden z klíčových parametrů.</span><span class="sxs-lookup"><span data-stu-id="7fefc-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="7fefc-114">Definuje, kolik aktuálního odhadu stochastického přechodu ovlivňuje aktualizaci parametru.</span><span class="sxs-lookup"><span data-stu-id="7fefc-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="7fefc-115">Velikost rozdílových hodnot aktualizace parametru je úměrná míře učení.</span><span class="sxs-lookup"><span data-stu-id="7fefc-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="7fefc-116">Menší hodnoty studijních kurzů vedou k pomalejšímu vývoji parametrů a pomalejším krokům, ale příliš velké hodnoty LR můžou sbližování zcela rozdělit, protože klesání na určitý místní minimum se nepotvrdí.</span><span class="sxs-lookup"><span data-stu-id="7fefc-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="7fefc-117">I když je rychlost učení adaptivní úpravou sledovacího algoritmu v určitém rozsahu, je důležité vybrat vhodnou počáteční hodnotu.</span><span class="sxs-lookup"><span data-stu-id="7fefc-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="7fefc-118">Obvyklá výchozí počáteční hodnota pro studijní frekvenci je 0,1.</span><span class="sxs-lookup"><span data-stu-id="7fefc-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="7fefc-119">Výběr nejlepší hodnoty studijní frekvence je jemný obrázek (například oddíl 4,3 z Goodfellow et al., "obsáhlý Learning", MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="7fefc-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="7fefc-120">Velikost Minibatch</span><span class="sxs-lookup"><span data-stu-id="7fefc-120">Minibatch size</span></span>

<span data-ttu-id="7fefc-121">Definuje, kolik ukázek dat se používá pro jeden odhad stochastického přechodu.</span><span class="sxs-lookup"><span data-stu-id="7fefc-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="7fefc-122">Větší hodnoty minibatch velikosti obvykle vedou k větší robustnosti a většímu monotónní konvergenci, ale mohou potenciálně zpomalit proces školení, protože náklady na jeden odhad přechodu jsou úměrné velikosti minimatch.</span><span class="sxs-lookup"><span data-stu-id="7fefc-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="7fefc-123">Obvyklá výchozí hodnota pro velikost minibatch je 10.</span><span class="sxs-lookup"><span data-stu-id="7fefc-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="7fefc-124">Školení epochs, tolerance, gridlocks</span><span class="sxs-lookup"><span data-stu-id="7fefc-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="7fefc-125">"Epocha" znamená jedno úplné předání prostřednictvím naplánovaných školicích dat.</span><span class="sxs-lookup"><span data-stu-id="7fefc-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="7fefc-126">Maximální počet epochs na školicí vlákno (viz níže) by měl být omezené.</span><span class="sxs-lookup"><span data-stu-id="7fefc-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="7fefc-127">Školicí vlákno je definováno k ukončení (s nejlépe známými parametry kandidáta), pokud byl spuštěn maximální počet epochs.</span><span class="sxs-lookup"><span data-stu-id="7fefc-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="7fefc-128">Nicméně takové školení bude ukončeno dříve, pokud nechybná míra klasifikace u plánu ověření klesne pod zvolenou toleranci.</span><span class="sxs-lookup"><span data-stu-id="7fefc-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="7fefc-129">Předpokládejme například, že chybná tolerance klasifikace je 0,01 (1%); Pokud se v sadě ověření 2000 ukázek zobrazuje méně než 20 chybných klasifikací, pak byla dosažena úroveň tolerance.</span><span class="sxs-lookup"><span data-stu-id="7fefc-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="7fefc-130">Školicí vlákno se také předčasně ukončí, pokud skóre ověření modelu kandidáta neukazuje žádné vylepšení více po sobě jdoucích epochs (Gridlock).</span><span class="sxs-lookup"><span data-stu-id="7fefc-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="7fefc-131">Logika pro ukončení Gridlock je aktuálně pevně zakódované.</span><span class="sxs-lookup"><span data-stu-id="7fefc-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="7fefc-132">Počet měření</span><span class="sxs-lookup"><span data-stu-id="7fefc-132">Measurements count</span></span>

<span data-ttu-id="7fefc-133">Odhad skóre pro školení a ověření a součásti stochastického přechodu na zařízení na základě počtu stavů se překrývají, což vyžaduje více měření odpovídajících observables.</span><span class="sxs-lookup"><span data-stu-id="7fefc-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="7fefc-134">Počet měření se má škálovat jako $O (1/\ Epsilon ^ 2) $ kde $ \epsilon $ je požadovaná chyba odhadu.</span><span class="sxs-lookup"><span data-stu-id="7fefc-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="7fefc-135">Jako pravidlo pro palec může být počáteční počet měření přibližně $1/\ mbox {tolerance} ^ 2 $ (viz definice tolerance v předchozím odstavci).</span><span class="sxs-lookup"><span data-stu-id="7fefc-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="7fefc-136">V takovém případě je potřeba zkontrolovat počet měření vzhůru, pokud se překlesání přechodu jeví jako příliš nestabilní a konvergence není příliš těžká k dosažení.</span><span class="sxs-lookup"><span data-stu-id="7fefc-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="7fefc-137">Školicí vlákna</span><span class="sxs-lookup"><span data-stu-id="7fefc-137">Training threads</span></span>

<span data-ttu-id="7fefc-138">Pravděpodobnost, která je přístupným nástrojem pro třídění, je velmi zřídka konvexní, což znamená, že má obvykle velké množství místních Optima v prostoru parametru, který může být významně odlišný od kvality.</span><span class="sxs-lookup"><span data-stu-id="7fefc-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="7fefc-139">Vzhledem k tomu, že se proces SGD dá konvergovat pouze k jednomu konkrétnímu optimálnímu, je důležité prozkoumat více počátečních vektorů parametrů.</span><span class="sxs-lookup"><span data-stu-id="7fefc-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="7fefc-140">Běžný postup ve strojovém učení je náhodným inicializací těchto počátečních vektorů.</span><span class="sxs-lookup"><span data-stu-id="7fefc-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="7fefc-141">Rozhraní API pro školení Q # akceptuje libovolné pole těchto počátečních vektorů, ale podkladový kód je prozkoumá sekvenčně.</span><span class="sxs-lookup"><span data-stu-id="7fefc-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="7fefc-142">Na vícejádrových počítačích nebo ve skutečnosti na jakékoli architektuře paralelního zpracování je vhodné provést několik volání rozhraní API pro školení Q # paralelně s různými inicializacemi parametrů napříč voláními.</span><span class="sxs-lookup"><span data-stu-id="7fefc-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="7fefc-143">Postup úpravy parametrů</span><span class="sxs-lookup"><span data-stu-id="7fefc-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="7fefc-144">V knihovně QML je nejlepší způsob, jak upravit parametry, přepsáním výchozích hodnot parametru UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="7fefc-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="7fefc-145">Chcete-li to provést, zavolejte ho pomocí funkce [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) a použijte operátor `w/` pro přepsání výchozích hodnot.</span><span class="sxs-lookup"><span data-stu-id="7fefc-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="7fefc-146">Pokud například chcete použít měření 100 000 a rychlost učení 0,01:</span><span class="sxs-lookup"><span data-stu-id="7fefc-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
